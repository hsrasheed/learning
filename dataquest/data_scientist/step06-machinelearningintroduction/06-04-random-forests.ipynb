{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "A random forest is a kind of ensemble model. Ensembles combine the predictions of multiple models to create a more accurate final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two decision trees with slightly different parameters:\n",
    "\n",
    "One with min_samples_leaf set to 2\n",
    "One with max_depth set to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# Set index_col to False to avoid pandas thinking that the first column is row indexes (it's age)\n",
    "income = pandas.read_csv(\"income.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_categorical_vars(df):\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_cols:\n",
    "        cat_col = pandas.Categorical(df[col])\n",
    "        df[col] = cat_col.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education_num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital_status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital_gain  capital_loss  hours_per_week  native_country high_income  \n",
      "0          2174             0              40   United-States       <=50K  \n",
      "1             0             0              13   United-States       <=50K  \n",
      "2             0             0              40   United-States       <=50K  \n",
      "3             0             0              40   United-States       <=50K  \n",
      "4             0             0              40            Cuba       <=50K  \n",
      "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
      "0   39          7   77516          9             13               4   \n",
      "1   50          6   83311          9             13               2   \n",
      "2   38          4  215646         11              9               0   \n",
      "3   53          4  234721          1              7               2   \n",
      "4   28          4  338409          9             13               2   \n",
      "\n",
      "   occupation  relationship  race  sex  capital_gain  capital_loss  \\\n",
      "0           1             1     4    1          2174             0   \n",
      "1           4             0     4    1             0             0   \n",
      "2           6             1     4    1             0             0   \n",
      "3           6             0     2    1             0             0   \n",
      "4          10             5     2    0             0             0   \n",
      "\n",
      "   hours_per_week  native_country  high_income  \n",
      "0              40              39            0  \n",
      "1              13              39            0  \n",
      "2              40              39            0  \n",
      "3              40              39            0  \n",
      "4              40               5            0  \n"
     ]
    }
   ],
   "source": [
    "#Convert all columns\n",
    "print(income.head(5))\n",
    "convert_categorical_vars(income)\n",
    "print(income.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "\n",
    "# Set a random seed so the shuffle is the same every time\n",
    "numpy.random.seed(1)\n",
    "\n",
    "# Shuffle the rows  \n",
    "# This permutes the index randomly using numpy.random.permutation\n",
    "# Then, it reindexes the dataframe with the result\n",
    "# The net effect is to put the rows into random order\n",
    "income = income.reindex(numpy.random.permutation(income.index))\n",
    "\n",
    "train_max_row = math.floor(income.shape[0] * .8)\n",
    "\n",
    "train = income[:train_max_row]\n",
    "test = income[train_max_row:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6878964226062301\n",
      "0.6759853906508785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "clf2 = DecisionTreeClassifier(random_state=1, max_depth=5)\n",
    "clf2.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))\n",
    "# 0.6878964226062301\n",
    "\n",
    "predictions = clf2.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))\n",
    "# 0.6759853906508785"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n",
    "\n",
    "* Majority voting\n",
    "* Algebraic combiners:\n",
    "    * Mean\n",
    "    * Sum\n",
    "    * Weighted Sum\n",
    "    * Product\n",
    "    * Maximum\n",
    "    * Minimum\n",
    "    * Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7150846804038882\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict_proba(test[columns])[:,1]\n",
    "predictions2 = clf2.predict_proba(test[columns])[:,1]\n",
    "combined = (predictions + predictions2) / 2\n",
    "rounded = numpy.round(combined)\n",
    "\n",
    "print(roc_auc_score(test[\"high_income\"], rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| settings\t| test AUC |\n",
    "|---|---|\n",
    "|min_samples_leaf: 2 |\t0.688 |\n",
    "|max_depth: 5 |\t0.676 |\n",
    "|combined predictions\t| 0.715 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The more \"diverse\" or dissimilar the models we use to construct an ensemble are, the stronger their combined predictions will be (assuming that all of the models have about the same accuracy). Ensembling a decision tree and a logistic regression model, for example, will result in stronger predictions than ensembling two decision trees with similar parameters. That's because those two models use very different approaches to arrive at their answers.\n",
    "\n",
    "* On the other hand, if the models we ensemble are very similar in how they make predictions, ensembling will result in a negligible boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A random forest is an ensemble of decision trees. If we don't make any modifications to the trees, each tree will be exactly the same, so we'll get no boost when we ensemble them. In order to make ensembling effective, we have to introduce variation into each individual decision tree model.\n",
    "* There are two main ways to introduce variation in a random forest -- bagging and random feature subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests - Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In a random forest, we don't train each tree on the entire data set. We train it on a random sample of the data, or a \"bag,\" instead. We perform this sampling with replacement, which means that after we select a row from the data we're sampling, we put the row back in the data so it can be picked again. Some rows from the original data may appear in the \"bag\" multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7329963297474371\n"
     ]
    }
   ],
   "source": [
    "# We'll build 10 trees\n",
    "tree_count = 10\n",
    "\n",
    "# Each \"bag\" will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    # We select 60% of the rows from train, sampling with replacement\n",
    "    # We set a random state to ensure we'll be able to replicate our results\n",
    "    # We set it to i instead of a fixed value so we don't get the same sample in every loop\n",
    "    # That would make all of our trees the same\n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\"\n",
    "    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "    clf.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data\n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "    \n",
    "combined = numpy.sum(predictions, axis=0) / 10\n",
    "rounded = numpy.round(combined)\n",
    "\n",
    "print(roc_auc_score(test[\"high_income\"], rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how splits work, let's look at information gain or entropy. Entropy is the measure of \"disorder\" in the data set. If a dataset has all the same labels, they'll have low entropy. If all the labels are different, they'll have high entropy. Splits that give us more information about the data, will ideally minimize entropy. In other words, the tree will ideally split the labels into distinct groups with as little mixture possible. This'll allow the splits to give our tree more predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_information_gain(data, split_name, target_name):\n",
    "    \"\"\"\n",
    "    Calculate information gain given a data set, column to split on, and target\n",
    "    \"\"\"\n",
    "    # Calculate the original entropy\n",
    "    original_entropy = calc_entropy(data[target_name])\n",
    "    \n",
    "    # Find the median of the column we're splitting\n",
    "    column = data[split_name]\n",
    "    median = column.median()\n",
    "    \n",
    "    # Make two subsets of the data, based on the median\n",
    "    left_split = data[column <= median]\n",
    "    right_split = data[column > median]\n",
    "    \n",
    "    # Loop through the splits and calculate the subset entropies\n",
    "    to_subtract = 0\n",
    "    for subset in [left_split, right_split]:\n",
    "        prob = (subset.shape[0] / data.shape[0]) \n",
    "        to_subtract += prob * calc_entropy(subset[target_name])\n",
    "    \n",
    "    # Return information gain\n",
    "    return original_entropy - to_subtract\n",
    "\n",
    "def calc_entropy(column):\n",
    "    \"\"\"\n",
    "    Calculate entropy given a pandas series, list, or numpy array.\n",
    "    \"\"\"\n",
    "    # Compute the counts of each unique value in the column\n",
    "    counts = numpy.bincount(column)\n",
    "    # Divide by the total column length to get a probability\n",
    "    probabilities = counts / len(column)\n",
    "    \n",
    "    # Initialize the entropy to 0\n",
    "    entropy = 0\n",
    "    # Loop through the probabilities, and add each one to the total entropy\n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            entropy += prob * math.log(prob, 2)\n",
    "    \n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': 1, 'column': 'age', 'median': 37.5, 'left': {'number': 2, 'column': 'employment', 'median': 4.0, 'left': {'number': 3, 'column': 'age', 'median': 22.5, 'left': {'number': 4, 'label': 0}, 'right': {'number': 5, 'label': 1}}, 'right': {'number': 6, 'label': 1}}, 'right': {'number': 7, 'column': 'age', 'median': 55.0, 'left': {'number': 8, 'column': 'age', 'median': 47.5, 'left': {'number': 9, 'label': 0}, 'right': {'number': 10, 'label': 1}}, 'right': {'number': 11, 'label': 0}}}\n"
     ]
    }
   ],
   "source": [
    "# Create the data set that we used two missions ago\n",
    "data = pandas.DataFrame([\n",
    "    [0,4,20,0],\n",
    "    [0,4,60,2],\n",
    "    [0,5,40,1],\n",
    "    [1,4,25,1],\n",
    "    [1,5,35,2],\n",
    "    [1,5,55,1]\n",
    "    ])\n",
    "data.columns = [\"high_income\", \"employment\", \"age\", \"marital_status\"]\n",
    "\n",
    "# Set a random seed to make the results reproducible\n",
    "numpy.random.seed(1)\n",
    "\n",
    "# The dictionary to store our tree\n",
    "tree = {}\n",
    "nodes = []\n",
    "\n",
    "# The function to find the column to split on\n",
    "def find_best_column(data, target_name, columns):\n",
    "    information_gains = []\n",
    "    \n",
    "    # Select two columns randomly\n",
    "    cols = numpy.random.choice(columns, 2)\n",
    "    \n",
    "    for col in cols:\n",
    "        information_gain = calc_information_gain(data, col, \"high_income\")\n",
    "        information_gains.append(information_gain)\n",
    "\n",
    "    highest_gain_index = information_gains.index(max(information_gains))\n",
    "    \n",
    "    # Get the highest gain by indexing \"cols\"\n",
    "    highest_gain = cols[highest_gain_index]\n",
    "    \n",
    "    return highest_gain\n",
    "\n",
    "# The function to construct an ID3 decision tree\n",
    "def id3(data, target, columns, tree):\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "    nodes.append(len(nodes) + 1)\n",
    "    tree[\"number\"] = nodes[-1]\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        if 0 in unique_targets:\n",
    "            tree[\"label\"] = 0\n",
    "        elif 1 in unique_targets:\n",
    "            tree[\"label\"] = 1\n",
    "        return\n",
    "    \n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    tree[\"column\"] = best_column\n",
    "    tree[\"median\"] = column_median\n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    split_dict = [[\"left\", left_split], [\"right\", right_split]]\n",
    "    \n",
    "    for name, split in split_dict:\n",
    "        tree[name] = {}\n",
    "        id3(split, target, columns, tree[name])\n",
    "\n",
    "\n",
    "# Run the ID3 algorithm on our data set and print the resulting tree\n",
    "id3(data, \"high_income\", [\"employment\", \"age\", \"marital_status\"], tree)\n",
    "print(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7345958637997538\n"
     ]
    }
   ],
   "source": [
    "# We'll build 10 trees\n",
    "tree_count = 10\n",
    "\n",
    "# Each \"bag\" will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    # We select 60% of the rows from train, sampling with replacement\n",
    "    # We set a random state to ensure we'll be able to replicate our results\n",
    "    # We set it to i instead of a fixed value so we don't get the same sample every time\n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\"\n",
    "    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2, splitter=\"random\", max_features=\"auto\")\n",
    "    clf.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data\n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "\n",
    "combined = numpy.sum(predictions, axis=0) / 10\n",
    "rounded = numpy.round(combined)\n",
    "\n",
    "print(roc_auc_score(test[\"high_income\"], rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7347461391939776\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=5, random_state=1, min_samples_leaf=2)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7347461391939776\n",
      "0.7379403213124711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=5, random_state=1, min_samples_leaf=2)\n",
    "\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=2)\n",
    "\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8192570489534683\n",
      "0.7139325899284541\n",
      "0.7917047295143252\n",
      "0.7498874343962398\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=5)\n",
    "\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(train[\"high_income\"], predictions))\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=5)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(train[\"high_income\"], predictions))\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
