{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Filter with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Goal**: Design a filter to detect spam SMS messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**: The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the authors' papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('SMSSpamCollection',sep='\\t',header=None,names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Label', 'SMS'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n"
     ]
    }
   ],
   "source": [
    "print(str(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dividing test and training data\n",
    "\n",
    "* Reserving 20% of the data for testing, 80% for training the model\n",
    "* We'll randomize the dataset before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_label_totals(df,column,labels):\n",
    "    for label in labels:\n",
    "        total_rows = len(df)\n",
    "        num_label = len(df[df[column] == label])\n",
    "        print(str(num_label)+\" out of \"+str(total_rows)+\" are label: \"+label+\" - \"+str((num_label/total_rows)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3855 out of 4457 are label: ham - 86.49315683194975%\n",
      "602 out of 4457 are label: spam - 13.506843168050258%\n",
      "970 out of 1115 are label: ham - 86.99551569506725%\n",
      "145 out of 1115 are label: spam - 13.004484304932735%\n"
     ]
    }
   ],
   "source": [
    "randomized_df = df.sample(frac=1,random_state=1)\n",
    "num_training_rows = int(len(df)*.8)\n",
    "training_set = df.iloc[:num_training_rows]\n",
    "test_set = df.iloc[num_training_rows:]\n",
    "print_label_totals(training_set,'Label',['ham','spam'])\n",
    "print_label_totals(test_set,'Label',['ham','spam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Preprocessing and Cleaning for the Training data\n",
    "\n",
    "Well need to do some encoding to map the dataset from using strings as messages, to using quantitative frequency measurements for how many times a given word was present in the message.\n",
    "\n",
    "This will enable us to calculate occurrence frequencies for individual words across the data set and then eventually probabilities.\n",
    "\n",
    "We'll also need to remove punctuation and standardize capitalization across all of the messages to simplify string matching for words later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.iloc[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005  text fa to 87121 to receive entry question std txt rate t c s apply 08452810075over18 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrasheed\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\hrasheed\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.replace('\\W',' ')\n",
    "training_set['SMS'] = training_set['SMS'].str.lower()\n",
    "## Alternative method\n",
    "## df.SMS.str.replace('[^a-zA-Z]',' ')\n",
    "print(training_set.iloc[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                                SMS\n",
      "0   ham  go until jurong point  crazy   available only ...\n",
      "1   ham                      ok lar    joking wif u oni   \n",
      "2  spam  free entry in 2 a wkly comp to win fa cup fina...\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrasheed\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                                SMS\n",
      "0   ham  [go, until, jurong, point, crazy, available, o...\n",
      "1   ham                     [ok, lar, joking, wif, u, oni]\n",
      "2  spam  [free, entry, in, 2, a, wkly, comp, to, win, f...\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['haunt', 'colleagues', 'exam', 'burial', 'hp', '09066364311', '09701213186', 'fast', 'mouth', 'oath', 'bcums', 'later', 'babes', 'motive', '2004', 'cmon', 'photo', 'medicine', 'official', 'file']\n"
     ]
    }
   ],
   "source": [
    "vocab_list = []\n",
    "for message in training_set['SMS']:\n",
    "    for word in message:\n",
    "        vocab_list.append(word)\n",
    "        \n",
    "vocabulary = list(set(vocab_list))\n",
    "print(vocabulary[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_sms = {}\n",
    "for vocab_word in vocabulary:\n",
    "    word_counts_per_sms[vocab_word] = [0]*len(training_set)\n",
    "    \n",
    "for index, sms in enumerate(training_set['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "        \n",
    "word_count_df = pd.DataFrame(word_counts_per_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new_training_set.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                                SMS  haunt  colleagues  \\\n",
      "0   ham  [go, until, jurong, point, crazy, available, o...      0           0   \n",
      "1   ham                     [ok, lar, joking, wif, u, oni]      0           0   \n",
      "2  spam  [free, entry, in, 2, a, wkly, comp, to, win, f...      0           0   \n",
      "\n",
      "   exam  burial  hp  09066364311  09701213186  fast  ...  headstart  hardcore  \\\n",
      "0     0       0   0            0            0     0  ...          0         0   \n",
      "1     0       0   0            0            0     0  ...          0         0   \n",
      "2     0       0   0            0            0     0  ...          0         0   \n",
      "\n",
      "   james  iraq  tsandcs  result  2px  suffers  less  irritating  \n",
      "0      0     0        0       0    0        0     0           0  \n",
      "1      0     0        0       0    0        0     0           0  \n",
      "2      0     0        0       0    0        0     0           0  \n",
      "\n",
      "[3 rows x 7811 columns]\n"
     ]
    }
   ],
   "source": [
    "df_new_training_set = pd.concat([training_set,word_count_df],axis=1,sort=False)\n",
    "print(df_new_training_set.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the Naive Bayes Algorithm\n",
    "\n",
    "### Theoretical basis\n",
    "\n",
    "We will use the following equations:\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{aligned}\n",
    "&N_{w_i|Spam} = \\text{the number of times the word } w_i \\text{ occurs in spam messages} \\\\\n",
    "&N_{w_i|Spam^C} = \\text{the number of times the word } w_i \\text{ occurs in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Spam} = \\text{total number of words in spam messages} \\\\\n",
    "&N_{Spam^C} = \\text{total number of words in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Vocabulary} = \\text{total number of words in the vocabulary} \\\\\n",
    "&\\alpha = 1 \\ \\ \\ \\ (\\alpha \\text{ is a smoothing parameter})\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_spam = len(df_new_training_set[df_new_training_set['Label'] == 'spam'])/len(df_new_training_set)\n",
    "print(p_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ham = len(df_new_training_set[df_new_training_set['Label'] == 'ham'])/len(df_new_training_set)\n",
    "print(p_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15341\n"
     ]
    }
   ],
   "source": [
    "n_spam = 0\n",
    "all_spam_df = df_new_training_set[df_new_training_set['Label'] == 'spam']\n",
    "for word_list in all_spam_df['SMS']:\n",
    "    n_spam += len(word_list)\n",
    "print(n_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57202\n"
     ]
    }
   ],
   "source": [
    "n_ham = 0\n",
    "all_ham_df = df_new_training_set[df_new_training_set['Label'] == 'ham']\n",
    "for word_list in all_ham_df['SMS']:\n",
    "    n_ham += len(word_list)\n",
    "print(n_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7809\n"
     ]
    }
   ],
   "source": [
    "n_vocabulary = len(vocabulary)\n",
    "print(n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize two dictionaries, where each key-value pair is a unique word (from our vocabulary) represented as a string, and the value is 0. We'll need one dictionary to store the parameters for P(wi|Spam), and the other for P(wi|Ham).\n",
    "spam_word_probabilities = {}\n",
    "ham_word_probabilities = {}\n",
    "\n",
    "for vocab_word in vocabulary:\n",
    "    spam_word_probabilities[vocab_word] = 0\n",
    "    ham_word_probabilities[vocab_word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the spam and the ham messages in the training set into two different DataFrames.\n",
    "spam_prob = df_new_training_set[df_new_training_set['Label'] == 'spam']\n",
    "ham_prob = df_new_training_set[df_new_training_set['Label'] == 'ham']\n",
    "\n",
    "# Iterate over the vocabulary, and, for each word, calculate P(wi|Spam) and P(wi|Ham)\n",
    "for vocab_word in vocabulary:\n",
    "    num_word_occurrences_spam = spam_prob[vocab_word].sum()\n",
    "    num_word_occurrences_ham = ham_prob[vocab_word].sum()\n",
    "    p_word_given_spam =  (num_word_occurrences_spam+alpha)/(n_spam+alpha*n_vocabulary)\n",
    "    p_word_given_ham =  (num_word_occurrences_ham+alpha)/(n_ham+alpha*n_vocabulary)\n",
    "    # update the probability value in the two dictionaries\n",
    "    spam_word_probabilities[vocab_word] = p_word_given_spam\n",
    "    ham_word_probabilities[vocab_word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_classify_message(complete_message):\n",
    "    all_words = complete_message.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
